\documentclass[landscape,a4paper]{extarticle}
\usepackage{caption}
\usepackage{multicol}
\usepackage[top=1em,bottom=1em,left=1em,right=1em]{geometry}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{microtype}
\usepackage{pdfpages}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{anyfontsize}
\usepackage[shortlabels]{enumitem}
\usepackage{graphicx, float}
\usepackage{ulem} % Using \uline{} instead of \underline{} will make the line closer to the word
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{mathrsfs}
% \usepackage{lmodern}
\usepackage[scaled=0.85]{FiraMono}

% Remove vertical space before and after all verbatim environments
\BeforeBeginEnvironment{verbatim}{\vspace{-\topsep}\vspace{-\partopsep}}
\AfterEndEnvironment{verbatim}{\vspace{-\topsep}\vspace{-\partopsep}}

\renewcommand{\familydefault}{\sfdefault}

% Configure image directory
\graphicspath{{images/}}

\newenvironment{Figure}
  {\par\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}
  
% Remove caption labels
\captionsetup{labelformat=empty,labelsep=none}

% No paragraph indent
\setlength{\parindent}{0pt}

% No spaces between list items, no left margins
\setlist[enumerate]{nosep, leftmargin=*}
\setlist[itemize]{nosep, leftmargin=*}

% No spaces before and after math mode
\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize%
    \setlength\abovedisplayskip{0pt}%
    \setlength\belowdisplayskip{0pt}%
    \setlength\abovedisplayshortskip{-8pt}%
    \setlength\belowdisplayshortskip{2pt}%
}

\setlength{\columnsep}{2pt}
\newcommand{\entails}{\models}

\begin{document}
\fontsize{6}{6}\selectfont
% \fontfamily{qcs}\selectfont
\begin{multicols*}{5}
    \textbf{\uline{CSPs}}

    \textbf{CSP Formulation}

    \begin{itemize}
        \item State representation
        \begin{itemize}
            \item Variables: \verb|X = {x_1, ... , x_n}|
            \item Domains: \verb|D = {d_1, ... , d_k}|
            \begin{itemize}
                \item Such that \verb|x_i| has domain \verb|d_i|
            \end{itemize}
            \item Initial state: all variables unassigned
            \item Intermediate state: partial assignment
        \end{itemize}
        \item Actions, costs, and transition
        \begin{itemize}
            \item Assignment of values (within domain) to variables
            \item Costs are unnecessary
        \end{itemize}
        \item Goal test
        \begin{itemize}
            \item Constraints: \verb|C = {c_1, ... , c_m}|
            \begin{itemize}
                \item Defined via a constraint langauge (algebra, logic, sets)
                \item Each \verb|c_i| corresponds to a requirement on some subset of \verb|X|
                \item Can be unary (\textbar scope\textbar = 1), binary (\textbar scope\textbar = 2), or global (\textbar scope\textbar $>$ 2)
            \end{itemize}
            \item Objective is a complete and consistent assignment
            \begin{itemize}
                \item Find a legal assignment $(y_i, ..., y_n)$ s.t.
                $y_i \in D_i\ \forall i \in [1, n)$
                \item Complete: all variables assigned values
                \item Consistent: all constraints in \verb|C| satisfied
            \end{itemize}
        \end{itemize}
    \end{itemize}

    \textbf{Algo}

    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{csp_algo.png}        
    \end{Figure}

    \textbf{Search tree size}

    At depth $l$: $(|X| - l) \cdot |d|$ states

    Total number of leaf states: 

    \[
        nm \times (n - 1) m \times (n - 2) m \times \ldots \times 2m \times m = n ! m^n
    \]

    where $n = |x|$ and $m = |d|$

    Order of variable assignments is not important, just consider assignments to one variable per level ($m ^ n$ leaves)

    \begin{Figure}
        \centering
        \includegraphics[width=0.7\linewidth]{csp_search_tree_size.jpg}
    \end{Figure}

    \textbf{Backtracking algorithm}

    \begin{verbatim}
def backtrack(csp, assgt) -> bool:
    if assgt is complete:
        return assgt
    var = select_unassigned_variable(csp, assgt)
    for val in order_dom_values(csp, var, assgt):
        if val is consistent with assgt:
            add {var = val} to assgt
            inferences = inference(csp, var, assgt)
            if inferences != failure:
                add inferences to csp
                result = backtrack(csp, assgt)
                if result != failure:
                    return result
                remove inferences from csp
            remove {var = value} from assgt
    return failure
    \end{verbatim}

    \textbf{Variable-order heuristics}

    \textbf{MRV (minimum remaining values)}
    
    \begin{itemize}
        \item Choose variable with fewest legal values
        \item Most constrained variable
        \item Sort by domain size, choose variable with smallest domain size
        \item Places larger subtrees closer to the root so that any invalid state found prunes a larger
        subtree $\Rightarrow$ eliminates larger subtrees earlier
    \end{itemize}

    \textbf{Degree Heuristic}

    \begin{itemize}
        \item To break ties in MRV heuristics
        \item Pick unassigned variable with most constraints (highest degree in constraint graph)
        \item This reduces the branching factor $b$
    \end{itemize}

    Recommended variable selection: MRV, then degree, then random

    \textbf{Value-order Heuristics}

    \textbf{LCV (least constraining value)}

    \begin{itemize}
        \item Choose the value that rules out the fewest choices from remaining domain values
        \begin{itemize}
            \item Given assignment of value \verb|v| to variable \verb|x'|, determine set of unassigned
            variables \verb|U| that share a constraint with \verb|x'| and pick \verb|v| that maximises sum of consistent
            domain sizes of variables in \verb|U|
        \end{itemize}
        \item Avoids failure by avoiding empty domains
    \end{itemize}

    With variables, fail first. With values, fail last. Cuz need to look at all variables
    but dn to look at all values.

    BUT if all solutions required, then value-ordering irrelevant

    \textbf{Forward Checking}

    Track remaining legal values for unassigned variables, terminate search when any variable has no legal values
    (based on constraints with recently assigned variable)

    \textbf{Constraint propagation}

    Inference step to ensure local consistency of all variables
    \begin{itemize}
        \item Traverse constraint graph to ensure variable at each node is consistent, eliminate all values in variable's
        domain that are not consistent with linked constraints
    \end{itemize}

    \textbf{Node-consistent (vertex-consistent)}
    \begin{itemize}
        \item Domain of a variable is consistent with its unary constraints
        \item Achieved through pre-processing step before backtracking
    \end{itemize}

    \textbf{Arc-consistent (edge-consistent)}
    \begin{itemize}
        \item Domain of a variable is consistent with its binary constraints
        \item The variable's domain value must have a partnering domain value in other variable that will
        satisfy the binary constraint
    \end{itemize}

    $X_i$ is arc-consistent w.r.t $X_j$ $\iff$ $\forall x \in D_i\ \exists y \in D_j$ s.t. binary
    constraint on arc $(X_i, X_j)$ is satisfied

    When checking arc $(X_a, X_b)$, remove values from $D_a$

    \textbf{AC-3 Algo}

    \begin{verbatim}
function ac3(csp) -> bool:
    queue = queue of all arcs in csp (both dirs)
    while queue:
        (X_i, X_j) = queue.pop()
        if revise(csp, X_i, X_j):
            if len(D_i) == 0: 
                return False
            for X_k in X_i.neighbors - {X_j}:
                queue.append((X_k, X_i))
    return True

function revise(csp, X_i, X_j):
    revised = False
    for x in D_i:
        if no value in D_j allows (x, y) to satisfy 
        the constraint between X_i and X_j:
            delete x from D_i
            revised = true
    return revised
    \end{verbatim}

    \textbf{Time complexity of AC-3}

    With $n$ variables, there are at most $2 \times {n \choose 2} \in O(n^2)$ directed arcs

    Each arc can be reinserted at most $d$ times because $X_i$ has hat most $d$ values to delete
    where $d$ is domain size

    Checking consistency of arc (\verb|revise()|) takes $O(d^2)$ time

    Overall time complexity: $O(n^2 \times d \times d^2)$ = $O(n^2 \times d^3)$

    \textbf{\uline{Advesarial search}}

    Assume 2 players, zero-sum game. MAX player wants to maximise value (our agent),
    MIN player wants to minimise value (opponent)

    \textbf{Formulating games}

    \begin{itemize}
        \item State representation: as per general search formulation
        \item \verb|TO-MOVE(s)|: returns \verb|p|, the player to move in state \verb|s|
        \item \verb|ACTIONS(s)|: legal moves in state \verb|s|
        \item \verb|RESULT(s, a)|: the transition model, returns resultant immediate state
        when taking action \verb|a| at state \verb|s|
        \item \verb|IS-TERMINAL(s)|: returns \verb|True| when game is over, \verb|False| otherwise
        \item \verb|UTILITY(s, p)|: defines a numeric value (score) for player \verb|p| when the
        game ends at terminal state \verb|s|
    \end{itemize}

    For zero-sum games, at terminal state \verb|s|,

    \verb|utility(MAX, s) + utility(MIN, s) = 0|. 

    Just use \verb|utility(MAX) = -utility(MIN)|

    \textbf{Strategies: optimal decisions via minimax}

    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{minimax.png}        
    \end{Figure}

    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{nim.png}        
    \end{Figure}

    Backwards induction used

    \textbf{Properties}
    \begin{itemize}
        \item Complete assuming finite game tree
        \item Optimal assuming optimal gameplay (opponent plays optimally)
        \item Time complexity $O(b^m)$
        \item Space complexity $O(bm)$
        \item Time polynomial to tree size
    \end{itemize}

    \textbf{Backwards Induction Issue}

    Game trees are massive, impossible to expand entire tree, must find ways to shrink search tree 

    \textbf{$\alpha$ - $\beta$ pruning}
    
    General idea: don't explore moves that would never be considered.
    
    $\alpha$ bounds MAX's values,
    $\beta$ bounds MIN's values.

    Prune subtrees that will never affect Minimax decision

    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{alpha_beta_example.png}
    \end{Figure}

    $S_3$ will be -7 or lower. MAX will not choose it because it already has $S_1$ or $S_2$
    which are higher than -7.

    \textbf{Minimax algo (a and b are alpha and beta)}

    \begin{verbatim}
def minimax(node, depth, isMax, a, b):
  if node is a leaf node:
    return value of node
  if isMax:
    bestVal = -INFINITY
    for each child node:
      value = minimax(node, depth+1, false, a, b)
      bestVal = max(bestVal, value)
      a = max(a, bestVal)
      if b <= a:
        break
    return bestVal
  else:
    bestVal = +INFINITY
    for each child node:
      value = minimax(node, depth+1, true, a, b)
      bestVal = min(bestVal, value)
      beta = min(beta, bestVal)
      if b <= a:
        break
    return bestVal
minimax(0, 0, true, -INFINITY, +INFINITY)
    \end{verbatim}

    Perfect ordering with alpha beta gives time complexity $O(b^{m/2})$
    Random ordering gives time complexity $O(b^{3m/4})$

    \textbf{$\alpha$ - $\beta$ pruning issues:}

    \begin{itemize}
        \item Issue: Depth of the tree could be very large
        \begin{itemize}
            \item Backwards induction only works with terminal states
            \item If depth is very deep, it takes very long before we can reach terminal state
        \end{itemize}
        \item Solution: Heuristic minimax
        \begin{itemize}
            \item Cutoff test: have a depth limit as to how deep we search for a terminal node
            \item Evaluation function: Estimates the expected utility at that state
            \item Run MINIMAX until depth $d$ then start using evaluation function to choose nodes
            \item Replaces \verb|is_terminal(s)| with \verb|cutoff_test(s, d)|.
            \item Replaces \verb|utility(s, p)| with \verb|eval(s, p)|.
        \end{itemize}
    \end{itemize}

    \textbf{\uline{Logical agents}}

    \textbf{Knowledge-based agents}

    \begin{itemize}
        \item Represent agent domain knowledge using logical formulas
        \item Main components of a logical agent are inference engine and knowledge base. Inference
        engine consists of domain-independent algorithms while knowedge base is domain-specific content
    \end{itemize}

    \textbf{KB agent function}

    \begin{verbatim}
def KB_agent(percept) -> action:
    persistent: KB, t (time)

    TELL(KB, MAKE_PERCEPT_SENTENCE(percept, t))
    action = ASK(KB, MAKE_ACTION_QUERY(t))
    TELL(KB, MAKE_ACTION_SENTENCE(action, t))
    t++
    return action
    \end{verbatim}

    \textbf{Inference via entailment}

    \textbf{Entailment}

    \textbf{Modelling:} $v$ models $\alpha$ (a sentence) if $\alpha$ is true under $v$

    \begin{itemize}
        \item $v$ corresponds to one set of value assignments
        \item $v$ corresponds to one instance of the environment (known part of a state)
        \item Let $M(\alpha)$ be the set of all models for $\alpha$
    \end{itemize}

    \textbf{Entailment ($\entails$): } One thing (right) follows from the other (left)
    \begin{itemize}
        \item $\alpha \entails \beta \iff M(\alpha) \subseteq M(\beta)$
        \item E.g. $\alpha$ = $q$ is prime, $\beta$ = ($q \text{ is odd)} \vee q = 2$
    \end{itemize}

    \textbf{Inference}

    Inference is deriving new knowledge from the KB

    KB: environment rules/laws and percepts

    \vspace{0.5em}

    \begin{align*}
        KB \entails \alpha &\implies M(KB) \subseteq M(\alpha)\\
        M(KB) \cup M(\alpha) &\equiv M(\alpha)\\
        M(KB) \cap M(\alpha) &\equiv M(KB)\\
        M(KB) \cap M(\neg \alpha) &\equiv \emptyset
    \end{align*}

    Given KB, try to infer $\alpha$, i.e. determine if $KB \entails \alpha$.

    If $KB \entails \alpha$, then $\alpha$ can be added to KB since $M(KB) \cap M(\alpha) \equiv M(KB)$

    \textbf{Soundness \& Completeness}

    \begin{itemize}
        \item $KB \vdash_A \alpha$
        \begin{itemize}
            \item Means ``sentence $\alpha$ is derived from KB by inference algorithm A''
        \end{itemize}
        \item Soundness
        \begin{itemize}
            \item A is sound if $KB \vdash_A \alpha \implies KB \entails \alpha$
            \item A will not infer nonsense.
        \end{itemize}
        \item Completeness
        \begin{itemize}
            \item A is complete if $KB \entails \alpha \implies KB \vdash_A \alpha$
            \item A can infer any sentence that KB entails
        \end{itemize} 
    \end{itemize}

    \textbf{Truth table enumeration}

    Draw truth table of KB and $\alpha$, KB entails $\alpha$ if whenever KB true, $\alpha$ true

    \begin{Figure}
        \centering
        \includegraphics[width=\linewidth]{tt_enumeration.png}
    \end{Figure}

    \textbf{Properties}

    \begin{itemize}
        \item $O(2^n)$ time complexity
        \item $O(n)$ space complexity
    \end{itemize}

    \textbf{Theorem proving methods}

    \textbf{Validity \& Satisfiability}
    
    \begin{itemize}
        \item Validity
        \begin{itemize}
            \item A sentence $\alpha$ is \textbf{valid} if it is true for all possible truth value assignments
            \item \textbf{tautology}
            \item $(KB \entails \alpha) \iff (KB \implies \alpha)$ (deduction theorem)
        \end{itemize}
        \item Satisfiability
        \begin{itemize}
            \item A sentence is \textbf{satisfiable} if it is true for some truth value assignment (i.e. a model exists for that sentence)
            \item A sentence is \textbf{unsatisfiable} if it is true for no truth value assignments
            \begin{itemize}
                \item \textbf{contradictions}
            \end{itemize}
            \item $(KB \entails \alpha) \iff (KB \wedge \neg \alpha)$ is unsatisfiable
            \begin{itemize}
                \item Definition of proof by contradiction
            \end{itemize}
        \end{itemize}
    \end{itemize}

    \textbf{Inference rules}

    \begin{itemize}
        \item And-Elimination: $a \wedge b \entails a,\ a \wedge b \entails b$
        \item Modus Ponens: $a \wedge (a \implies b) \entails b$
        \item Logical equivalences: $(a \vee b) \entails \neg(\neg a \wedge \neg b)$
    \end{itemize}

    \textbf{CNF}

    CNF = conjunction of disjunctive sentences, e.g. $(x_1 \vee x_2) \wedge (x_2 \vee x_3 \vee x_4)$

    \textbf{Conversion to CNF}
    \begin{itemize}
        \item Convert:
        \begin{itemize}
            \item $\alpha \iff \beta$ to $(\alpha \implies \beta) \wedge (\beta \implies \alpha)$
        \end{itemize}
    \end{itemize}

    \textbf{Resolution}

    If a literal $x$ appears in $R_i$ and its negation $\neg x$ appears in $R_j$, 
    where $R_i, R_j \in KB$, then it can be removed from both. Then the remaining literals
    combine to 1 clause



\end{multicols*}
\end{document}